{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data wrangling project is the second project of the ALX-T Data Analyst Nanodegree programme on Udacity. The project focused on wrangling and analysing data from @WeRateDogs Twitter account. \n",
    "\n",
    "This involved:\n",
    "- gathering data from multiple sources (downloaded @WeRateDogs' Twitter archive dataset and image predictions data from URLs provided to Udacity by @WeRateDogs, and queried Twitter API for additional data that wasn't in the Twitter archive), \n",
    "- assessing (visually and programmatically), \n",
    "- cleaning and merging the datasets, and then \n",
    "- performing analysis on the tweets to extract insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step required me to gather three datasets:\n",
    "- @WeRateDogs' Twitter archive dataset (twitter-archive-enhanced.csv),\n",
    "- Tweet image predictions (image_predictions.tsv), and\n",
    "- additional data from Twitter API\n",
    "\n",
    "The first dataset, `twitter-archive-enhanced.csv`, was provided by Udacity. All I had to do was manually download it via a URL that was provided, upload it to the Jupyter Notebook folder, and read it into the pandas DataFrame using pd.read_csv()\n",
    "\n",
    "The second dataset, `image_predictions.tsv`, was programmatically downloaded using the Requests library and a URL that was provided by Udacity. I subsequently read it into the pandas DataFrame, employing io.IOString to read the contents of the requests.Response object. \n",
    "\n",
    "Finally, I queried Twitter API using tweepy to gather additional data (retweet_count and favorite_count) about the tweets in the @WeRateDogs' Twitter archive dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for quality and tidiness issues, I assessed each of three datasets:\n",
    "- visually (printed the entire dataset -- as much of it as Jupyter notebook would allow -- and scrolled through it), and\n",
    "- programmatically (using .head(), .sample(), .info(), .describe(), etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found 13 issues in the data assessing process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Issue ID | Table | Issue Type | Column | Description |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | `archive_df` | Quality | `name` | Invalid dog names e.g. \"a\", \"then\", etc. |\n",
    "| 2 | `archive_df` | Quality | `rating_denominator` | There were invalid denominators (had values other than 10) |\n",
    "| 3 | `archive_df` | Quality | `source` | Contains href. Extract href string |\n",
    "| 4 | `archive_df` | Quality | `text` | Irrelevant column |\n",
    "| 5 | `archive_df` | Quality | `retweeted_status_id` | Same dog recorded twice or more |\n",
    "| 6 | `archive_df` | Quality | `in_reply_to_status_id` | Same dog recorded twice or more |\n",
    "| 7 | `archive_df` | Quality | `timestamp` | Convert to `datetime` |\n",
    "| 8 | `archive_df` | Quality | `doggo`, `floofer`, `pupper`, `puppo` | Convert to `category` datatype |\n",
    "| 9 | `archive_df` | Quality | `tweet_id` | Convert to `string` datatype |\n",
    "| 10 | `img_prediction_df` | Quality | `img_num` | Irrelevant column |\n",
    "| 11 | `img_prediction_df` | Quality | `jpg_url` | Duplicate values |\n",
    "| 12 | `archive_df`, `img_prediction_df`, `tweets_df` | Tidiness| `tweet_id` | Merge tables into one |\n",
    "| 13 | `archive_df` | Tidiness | `doggo`, `floofer`, `pupper`, `puppo` | Merge columns into one, `dog_stage` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before cleaning, I first made copies of the original datasets. \n",
    "\n",
    "During cleaning, I ensured to drop all rows that did not contain original tweets (e.g. retweets). This would ensure that the same dog wouldn't be recorded more than once. I fixed the other quality and tidiness issues listed in the table above using the define-code-test methodology. \n",
    "\n",
    "Finally, I merged the datasets into one dataset, `df_merged_clean`, which was ready for analysis and visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned dataset, `df_merged_clean`, was saved to a csv file, `twitter_archive_master.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assessed, documented, and addressed 13 issues (11 quality issues and 2 tidiness issues). Then, I saved the final dataset (which contains 1851 observations and 19 variables) as `twitter_archive_master.csv`.\n",
    "\n",
    "While the wrangling done was satisfactory for my project, the final dataset isn't devoid of issues. For example, there were null values (written as None) in `dog_stage` and `name` columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
